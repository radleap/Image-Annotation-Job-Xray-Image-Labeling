{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "## Project: Machine Learning \n",
    "\n",
    "---\n",
    "## Domain Background & Problem Statement\n",
    "\n",
    "**Domain Background**\n",
    "\n",
    "Computer vision (image recognition) in the healthcare field.\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "To correctly classify images as normal vs pneumonia using machine learning, initial metric is accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Layout\n",
    "\n",
    "We break the notebook into separate steps. These links navigate the notebook.\n",
    "\n",
    "* [Step 1](#import): Obtaining Data\n",
    "* [Step 2](#eda): Exploratory Data Analysis\n",
    "* [Step 3](#modeling): Modeling\n",
    "    * [Model 1](#NN_baseline):  Neural Network, 2 layers, baseline model\n",
    "    * [Model 2](#NN): Neural Network, complexity  \n",
    "    * [Model 3](#CNN): Convolutional Neural Network\n",
    "    * [Model 4](#transfer): Transfer Learning NN\n",
    "* [Step 99](#resources): Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "%matplotlib inline   \n",
    "\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filenames for human and dog images\n",
    "normal_files = np.array(glob(\"data/chest_xray/train/NORMAL/*\"))\n",
    "pneumonia_files = np.array(glob(\"data/chest_xray/train/pneumonia/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total normal x_ray images in the training set.' % len(normal_files)) # normal_files = human_files\n",
    "print('There are %d total pneumonia x_ray images in the training set.' % len(pneumonia_files)) # pneumonia_files = dog_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filenames for human and dog images\n",
    "normal_files = np.array(glob(\"data/chest_xray/val/NORMAL/*\"))\n",
    "pneumonia_files = np.array(glob(\"data/chest_xray/val/pneumonia/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total normal x_ray images in the validation set.' % len(normal_files)) # normal_files = human_files\n",
    "print('There are %d total pneumonia x_ray images in the validation set.' % len(pneumonia_files)) # pneumonia_files = dog_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an imbalanced dataset between the training and validation, introducing bias. As a result, the imbalance was improved to a near 80/20 split due to this large imbalance. \n",
    "\n",
    "The original download has only 16 images in validation and 5216 in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "The following shows the diversity within each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "w=300\n",
    "h=80\n",
    "fig=plt.figure(figsize=(20, 20))\n",
    "columns = 3\n",
    "rows = 1\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = Image.open(pneumonia_files[i*10])\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img, cmap = plt.cm.gray)\n",
    "    plt.title(i*10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining just a few images there are some important elements to note. Much less .... image 3 not as clear...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=300\n",
    "h=80\n",
    "fig=plt.figure(figsize=(20, 20))\n",
    "columns = 3\n",
    "rows = 1\n",
    "for i in range(1, columns*rows +1):\n",
    "    img = Image.open(normal_files[i*10])\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img, cmap = plt.cm.gray)\n",
    "    plt.title(i*10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2     \n",
    "\n",
    "width = []\n",
    "height = []\n",
    "channels = []\n",
    "\n",
    "for i in range(len(pneumonia_files)):\n",
    "    img = cv2.imread(pneumonia_files[i])\n",
    "    dimensions = img.shape\n",
    "    width.append(dimensions[0])\n",
    "    height.append(dimensions[1])\n",
    "    channels.append(dimensions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(width, height, channels)), columns = ['Width (pixels)', 'Height (pixels)','Channels (RGB)'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the range of input images in our data that we will use to train, validate, and test the model. All images are three channels, which indicates RGB, but is in grayscale, so not sure why not 2 channels. The \"width\" dimension has an average of 825 pixels and \"height\" dimension of 1200 pixels. However, statistically speaking, there is a lot of variation in the data. So, what does this mean to us? Some images have more information than others. Preprocessing steps may affect some images differently, and resizing will be necessary. (not this is only the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Define figure and subplot\n",
    "new_figure = plt.figure(figsize=(14,4))\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "ax.boxplot(df['Width (pixels)'], vert = False)\n",
    "ax.set_title('Boxplot: Width of training data images (pixels)')\n",
    "ax.set_xlabel('Width (pixels)')\n",
    "\n",
    "ax2.boxplot(df['Height (pixels)'], vert = False)\n",
    "ax2.set_title('Boxplot: Height of training data images (pixels)')\n",
    "ax2.set_xlabel('Height (pixels)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'data/chest_xray/train'\n",
    "validation_dir = 'data/chest_xray/val'\n",
    "test_dir = 'data/chest_xray/test'\n",
    "\n",
    "num_samples = len(pneumonia_files) + len(normal_files)\n",
    "batch_size = 24\n",
    "\n",
    "# All images will be rescaled by 1./255, data augmentation for training dataset\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255) # 255 for scaling the 0-256 RGB values\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range = 10, #10 degree rotation\n",
    "                                  zoom_range=0.2, #zoom up to 20%\n",
    "                                  shear_range=0.1 #rotation plan 10%\n",
    "                                  ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),         # All images will be resized to 224X224\n",
    "        batch_size= batch_size, \n",
    "        class_mode='binary') #binary_crossentropy loss, we need binary labe\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size= batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# test generator\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_dir, \n",
    "        target_size=(224, 224), \n",
    "        batch_size= 1,\n",
    "        shuffle= False,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='modeling'></a>\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NN_baseline'></a>\n",
    "# Baseline: Neural Network, MLP\n",
    "- no convolutions\n",
    "- simple, with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "epochs = 5\n",
    "early_stopping_monitor = EarlyStopping(patience=2) # 2 epochs no improvement\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu',input_shape=(224, 224, 3)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.0001) # decreased learning rate due to oscillating \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Set the model to train; see warnings above\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= num_samples // batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[early_stopping_monitor])\n",
    "\n",
    "# saving the model\n",
    "model.save('static/artifacts/chest_xray_ann_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualing the model loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Define figure and subplot\n",
    "new_figure = plt.figure(figsize=(14,4))\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Loss Plot\n",
    "ax.plot(train_losses,  color='blue', linewidth=3, linestyle = '-')\n",
    "ax.plot(val_losses,  color='orange', linewidth=3, linestyle = '-')\n",
    "ax.set_title('Loss: Training vs Validation')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(('Train','Valid'), frameon = False)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(acc,  color='blue', linewidth=3, linestyle = '-.')\n",
    "ax2.plot(val_acc,  color='orange', linewidth=3, linestyle = '-.')\n",
    "ax2.set_title('Accuracy: Training vs Validation')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(('Train','Valid'), frameon = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(test_generator,\n",
    "                                 steps = 624, # steps = num_samples / batch_size,\n",
    "                                 workers = 1,\n",
    "                                 pickle_safe=False)\n",
    "    \n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, steps = 624)\n",
    "print(scores)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, steps = 624)\n",
    "\n",
    "y_true=test_generator.classes\n",
    "print(y_true.shape)\n",
    "y_pred1 = np.rint(y_pred) #rounding at 0.5 cutoff\n",
    "print(y_pred1.shape)\n",
    "\n",
    "print(classification_report(y_true, y_pred1, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='NN'></a>\n",
    "# Neural Network, MLP\n",
    "- no convolutions\n",
    "- Additional layers, added dropout as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout \n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(224, 224, 3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= optimizer, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Set the model to train; see warnings above\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= num_samples / batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose = 1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[early_stopping_monitor])\n",
    "\n",
    "# saving the model\n",
    "model.save('static/artifacts/chest_xray_ann_better_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Define figure and subplot\n",
    "new_figure = plt.figure(figsize=(14,4))\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Loss Plot\n",
    "ax.plot(train_losses,  color='blue', linewidth=3, linestyle = '-')\n",
    "ax.plot(val_losses,  color='orange', linewidth=3, linestyle = '-')\n",
    "ax.set_title('Loss: Training vs Validation')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(('Train','Valid'), frameon = False)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(acc,  color='blue', linewidth=3, linestyle = '-.')\n",
    "ax2.plot(val_acc,  color='orange', linewidth=3, linestyle = '-.')\n",
    "ax2.set_title('Accuracy: Training vs Validation')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(('Train','Valid'), frameon = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, steps = 624)\n",
    "print(scores)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, steps = 624)\n",
    "\n",
    "y_true=test_generator.classes\n",
    "print(y_true.shape)\n",
    "y_pred1 = np.rint(y_pred) #rounding at 0.5 cutoff\n",
    "print(y_pred1.shape)\n",
    "\n",
    "print(classification_report(y_true, y_pred1, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='CNN'></a>\n",
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "    \n",
    "early_stopping_monitor = EarlyStopping(patience=2) # 2 epochs no improvement\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= num_samples / batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose = 1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[early_stopping_monitor])\n",
    "\n",
    "#saving the model\n",
    "model.save('static/artifacts/chest_xray_cnn_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Define figure and subplot\n",
    "new_figure = plt.figure(figsize=(14,4))\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Loss Plot\n",
    "ax.plot(train_losses,  color='blue', linewidth=3, linestyle = '-')\n",
    "ax.plot(val_losses,  color='orange', linewidth=3, linestyle = '-')\n",
    "ax.set_title('Loss: Training vs Validation')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(('Train','Valid'), frameon = False)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(acc,  color='blue', linewidth=3, linestyle = '-.')\n",
    "ax2.plot(val_acc,  color='orange', linewidth=3, linestyle = '-.')\n",
    "ax2.set_title('Accuracy: Training vs Validation')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(('Train','Valid'), frameon = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, steps = 624)\n",
    "print(scores)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, steps = 624)\n",
    "\n",
    "y_true=test_generator.classes\n",
    "print(y_true.shape)\n",
    "y_pred1 = np.rint(y_pred) #rounding at 0.5 cutoff\n",
    "print(y_pred1.shape)\n",
    "\n",
    "print(classification_report(y_true, y_pred1, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='transfer'></a>\n",
    "# Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Base\n",
    "from keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(224, 224, 3))\n",
    "\n",
    "#Define Model Architecture\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_base.trainable = False\n",
    "\n",
    "#You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute:\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "#Similarly, we can check how many trainable weights are in the model:\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilation\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Set the model to train; see warnings above\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= num_samples / batch_size,\n",
    "      epochs=epochs,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[early_stopping_monitor])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= optimizer, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Set the model to train; see warnings above\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch= num_samples / batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose = 1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "# Define figure and subplot\n",
    "new_figure = plt.figure(figsize=(14,4))\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Loss Plot\n",
    "ax.plot(train_losses,  color='blue', linewidth=3, linestyle = '-')\n",
    "ax.plot(val_losses,  color='orange', linewidth=3, linestyle = '-')\n",
    "ax.set_title('Loss: Training vs Validation')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend(('Train','Valid'), frameon = False)\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(acc,  color='blue', linewidth=3, linestyle = '-.')\n",
    "ax2.plot(val_acc,  color='orange', linewidth=3, linestyle = '-.')\n",
    "ax2.set_title('Accuracy: Training vs Validation')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(('Train','Valid'), frameon = False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, steps = 624)\n",
    "print(scores)\n",
    "\n",
    "y_pred = model.predict_generator(test_generator, steps = 624)\n",
    "\n",
    "y_true=test_generator.classes\n",
    "print(y_true.shape)\n",
    "y_pred1 = np.rint(y_pred) #rounding at 0.5 cutoff\n",
    "print(y_pred1.shape)\n",
    "\n",
    "print(classification_report(y_true, y_pred1, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='resources'></a>\n",
    "# Resources\n",
    "\n",
    "- https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "- https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf\n",
    "- https://medium.com/swlh/how-to-use-smote-for-dealing-with-imbalanced-image-dataset-for-solving-classification-problems-3aba7d2b9cad\n",
    "- https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n",
    "- https://stackoverflow.com/questions/52270177/how-to-use-predict-generator-on-new-images-keras?noredirect=1&lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
